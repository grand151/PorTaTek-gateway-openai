# Emulator API OpenAI z u≈ºyciem darmowych modeli

Ten projekt to gateway (proxy), kt√≥ry emuluje interfejs API OpenAI, przekierowujƒÖc zapytania do darmowych modeli dostƒôpnych przez OpenRouter oraz Google Gemini API.

## Funkcje

- üîÑ Pe≈Çna emulacja API OpenAI (drop-in replacement)
- üÜì Wykorzystanie darmowych modeli z OpenRouter i Google Gemini
- üó∫Ô∏è Automatyczne mapowanie modeli GPT na darmowe alternatywy
- üöÄ Wsparcie dla emulacji GPT-5 i najnowszych modeli
- üîÑ Automatyczny retry w przypadku b≈Çƒôd√≥w
- üîÄ Fallback do alternatywnych modeli w przypadku awarii
- üíæ Cachowanie odpowiedzi dla oszczƒôdno≈õci czasu i zasob√≥w
- üñºÔ∏è Obs≈Çuga modeli multimodalnych (tekst + obrazy)
- ü§ñ Wsparcie dla wielu provider√≥w (OpenRouter, Google Gemini)
- üéõÔ∏è Panel konfiguracyjny z interfejsem webowym

## Dostƒôpne modele

### Modele z OpenRouter (darmowe)

#### DeepSeek (najnowsze, wydajne)
- **deepseek-r1-0528** - 164K context, ≈õwietny dla reasoning i dialogu

#### Qwen (multimodalne, kodowanie)
- **qwen3-235b** - 262K context, zaawansowane reasoning
- **qwen3-next-80b** - 262K context, szybki i wszechstronny
- **qwen3-coder** - specjalizowany w kodowaniu
- **qwen3-vl-235b-thinking** - model wizyjny z thinking
- **qwen3-vl-30b-thinking** - model wizyjny, l≈ºejszy

#### Mistral AI
- **mistral-small-3.1-24b** - 128K context, vision, narzƒôdzia
- **mistral-small-2501** - nowa wersja, fallback
- **mistral-embed** - embeddings

#### Meta Llama
- **llama-3.3-70b** - du≈ºy model, wysokiej jako≈õci
- **llama-3.2-3b** - szybki, lekki model

#### Google Gemma
- **gemma-2-9b** - open source, uniwersalny
- **gemma-2-2b** - najmniejszy, najszybszy

#### OpenCode (nowe modele)
- **opencode-big-pickle** - model opencode Big Pickle
- **opencode-glm-5** - model opencode GLM-5
- **opencode-gpt-5-nano** - model opencode GPT-5 Nano
- **opencode-kimi-k2.5** - model opencode Kimi K2.5
- **opencode-minimax-m2.5** - model opencode Minimax M2.5

### Modele Google Gemini (bezpo≈õrednie API)

- **gemini-3-flash** - najnowszy, ultraszybki
- **gemini-3-pro** - najlepszy reasoning i analiza
- **gemini-2.0-flash** - do 1M token√≥w context
- **gemini-1.5-flash** - szybki, 128K context
- **gemini-1.5-pro** - zaawansowany, 1M context

## Mapowanie modeli

| Model OpenAI | Model docelowy | Provider |
|--------------|----------------|----------|
| gpt-3.5-turbo | DeepSeek R1 | OpenRouter |
| gpt-4 | DeepSeek R1 | OpenRouter |
| gpt-4o | Qwen3 235B | OpenRouter |
| gpt-4o-mini | Qwen3 Next 80B | OpenRouter |
| **gpt-5** | **Qwen3 235B** | **OpenRouter** |
| **gpt-5-turbo** | **Qwen3 Next 80B** | **OpenRouter** |
| **gpt-5-nano** | **OpenCode GPT-5 Nano** | **OpenRouter** |
| **gpt-5-preview** | **Qwen3 235B** | **OpenRouter** |
| gpt-4-vision | Qwen3 VL 235B | OpenRouter |
| gpt-4-code | Qwen3 Coder | OpenRouter |
| opencode-big-pickle | OpenCode Big Pickle | OpenRouter |
| opencode-glm-5 | OpenCode GLM-5 | OpenRouter |
| opencode-gpt-5-nano | OpenCode GPT-5 Nano | OpenRouter |
| opencode-kimi-k2.5 | OpenCode Kimi K2.5 | OpenRouter |
| opencode-minimax-m2.5 | OpenCode Minimax M2.5 | OpenRouter |
| gemini-3-flash | Gemini 3 Flash | Google Gemini |
| gemini-1.5-pro | Gemini 1.5 Pro | Google Gemini |
| text-embedding-ada-002 | Mistral Embed | OpenRouter |


## Wymagania

- Node.js 14+
- npm lub yarn
- Klucz API OpenRouter (opcjonalny, je≈õli u≈ºywasz Gemini)
- Klucz API Google Gemini (opcjonalny, je≈õli u≈ºywasz OpenRouter)

**Uwaga:** Wymagany jest przynajmniej jeden klucz API (OpenRouter lub Gemini).

## Uzyskanie kluczy API

### OpenRouter API Key (darmowy)
1. Zarejestruj siƒô na [https://openrouter.ai](https://openrouter.ai)
2. Przejd≈∫ do ustawie≈Ñ konta
3. Wygeneruj nowy klucz API
4. Darmowe modele majƒÖ limity: ~20 req/min, ~200 req/dzie≈Ñ

### Google Gemini API Key (darmowy)
1. Odwied≈∫ [https://aistudio.google.com](https://aistudio.google.com)
2. Zaloguj siƒô kontem Google
3. Kliknij "Get API Key" lub "Create API key"
4. Skopiuj i bezpiecznie zapisz klucz
5. Darmowy tier: ~15 req/min (Flash), ~2-5 req/min (Pro)

## Instalacja

```bash
# Klonowanie repozytorium
git clone <repo-url>
cd openai-gateway

# Instalacja zale≈ºno≈õci
npm install

# Konfiguracja zmiennych ≈õrodowiskowych
cp .env.example .env
# Edytuj plik .env z twoimi ustawieniami
```

## Uruchomienie

```bash
# Standardowe uruchomienie
npm start

# Tryb deweloperski z automatycznym restartem
npm run dev
```

Gateway bƒôdzie dostƒôpny pod adresem `http://localhost:8787`.

## Panel Konfiguracyjny

Gateway posiada wbudowany panel konfiguracyjny dostƒôpny pod adresem `http://localhost:8787/admin`.

### Funkcje panelu:
- üìä **Dashboard** - Status systemu, providery API, statystyki
- ü§ñ **Modele** - PrzeglƒÖdanie i dodawanie mapowa≈Ñ modeli
- üîÑ **Fallbacki** - Lista ≈Ça≈Ñcuch√≥w fallback√≥w
- ‚öôÔ∏è **Konfiguracja** - Pe≈Çny widok konfiguracji JSON
- üì° **API Docs** - Dokumentacja endpoint√≥w i przyk≈Çady u≈ºycia

### ZarzƒÖdzanie modelami przez API:

```bash
# Dodanie nowego mapowania modelu
curl -X POST "http://localhost:8787/config/models" \
  -H "Content-Type: application/json" \
  -d '{
    "openaiModel": "gpt-5-custom",
    "targetModel": "qwen/qwen3-235b-a22b:free",
    "provider": "openrouter"
  }'

# Pobranie aktualnej konfiguracji
curl "http://localhost:8787/config"

# Wyczyszczenie cache
curl -X POST "http://localhost:8787/config/clear-cache"
```

**Uwaga:** Zmiany konfiguracji przez panel sƒÖ tymczasowe (tylko w pamiƒôci). Po restarcie serwera, konfiguracja wraca do warto≈õci domy≈õlnych.

## U≈ºycie

Mo≈ºesz u≈ºywaƒá tego gateway dok≈Çadnie tak samo jak normalnego API OpenAI. Gateway automatycznie wykrywa i kieruje ≈ºƒÖdania do odpowiedniego providera.

### Przyk≈Çad z modelami OpenRouter

```bash
curl -X POST "http://localhost:8787/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-4",
    "messages": [
        {"role": "user", "content": "Wyja≈õnij jak dzia≈Ça AI"}
    ]
  }'
```

### Przyk≈Çad z modelami Gemini

```bash
curl -X POST "http://localhost:8787/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gemini-3-flash",
    "messages": [
        {"role": "user", "content": "Co to jest machine learning?"}
    ]
  }'
```

### Przyk≈Çad z modelami kodowania

```bash
curl -X POST "http://localhost:8787/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-4-code",
    "messages": [
        {"role": "user", "content": "Napisz funkcjƒô sortujƒÖcƒÖ w Python"}
    ]
  }'
```

### Przyk≈Çad z modelami OpenCode

```bash
curl -X POST "http://localhost:8787/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "opencode-big-pickle",
    "messages": [
        {"role": "user", "content": "Wyja≈õnij koncepcjƒô machine learning"}
    ]
  }'
```

### Przyk≈Çad z modelami GPT-5 (emulacja)

```bash
# GPT-5 (mapowany na Qwen3 235B)
curl -X POST "http://localhost:8787/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-5",
    "messages": [
        {"role": "user", "content": "Co nowego w AI w 2026?"}
    ]
  }'

# GPT-5 Turbo (mapowany na Qwen3 Next 80B)
curl -X POST "http://localhost:8787/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-5-turbo",
    "messages": [
        {"role": "user", "content": "Szybka odpowied≈∫ na pytanie"}
    ]
  }'

# GPT-5 Nano (mapowany na OpenCode GPT-5 Nano)
curl -X POST "http://localhost:8787/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-5-nano",
    "messages": [
        {"role": "user", "content": "Lekki i szybki model"}
    ]
  }'
```

### Python (OpenAI SDK)

```python
from openai import OpenAI

# Inicjalizacja klienta z nowym base URL
client = OpenAI(
    base_url="http://localhost:8787/v1",
    api_key="dowolny-string"  # klucz nie jest sprawdzany
)

# U≈ºycie dok≈Çadnie jak z OpenAI
response = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[
        {"role": "user", "content": "Twoje pytanie"}
    ]
)

print(response.choices[0].message.content)
```

### JavaScript/TypeScript

```javascript
import OpenAI from "openai";

const client = new OpenAI({
  baseURL: "http://localhost:8787/v1",
  apiKey: "dowolny-string" // klucz nie jest sprawdzany
});

const response = await client.chat.completions.create({
  model: "gpt-3.5-turbo",
  messages: [
    { role: "user", content: "Twoje pytanie" }
  ]
});

console.log(response.choices[0].message.content);
```

## Dostƒôpne endpointy

- `/v1/chat/completions` - generowanie odpowiedzi czatu
- `/v1/embeddings` - generowanie embedding√≥w
- `/v1/models` - lista dostƒôpnych modeli
- `/health` - sprawdzenie statusu serwera
- `/` - informacje o gateway
- `/admin` - panel konfiguracyjny (interfejs webowy)
- `/config` - pobieranie konfiguracji (JSON)
- `/config/models` - zarzƒÖdzanie mapowaniem modeli (POST)
- `/config/clear-cache` - czyszczenie cache (POST)

## Docker

```bash
# Budowanie obrazu
docker build -t openai-gateway .

# Uruchomienie
docker run -p 8787:8787 --env-file .env openai-gateway
```

Mo≈ºesz te≈º u≈ºyƒá docker-compose:

```bash
docker-compose up -d
```

## Zmienne ≈õrodowiskowe

| Zmienna | Opis | Domy≈õlna warto≈õƒá |
|---------|------|------------------|
| PORT | Port na kt√≥rym dzia≈Ça serwer | 8787 |
| OPENROUTER_API_KEY | Klucz API do OpenRouter | (opcjonalny*) |
| GEMINI_API_KEY | Klucz API do Google Gemini | (opcjonalny*) |
| CACHE_TTL | Czas ≈ºycia cache w milisekundach | 3600000 (1h) |
| MAX_RETRIES | Maksymalna liczba ponownych pr√≥b | 3 |
| RETRY_DELAY | Op√≥≈∫nienie miƒôdzy pr√≥bami (ms) | 1000 |

*Wymagany przynajmniej jeden z kluczy API (OPENROUTER_API_KEY lub GEMINI_API_KEY)

## Limity i ograniczenia

### OpenRouter
- Darmowe modele mogƒÖ byƒá wolniejsze ni≈º oryginalne modele OpenAI
- Limity: ~20 zapyta≈Ñ/minutƒô, ~200 zapyta≈Ñ/dzie≈Ñ
- Dostƒôpno≈õƒá zale≈ºy od OpenRouter

### Google Gemini
- Gemini Flash: ~15 zapyta≈Ñ/minutƒô
- Gemini Pro: ~2-5 zapyta≈Ñ/minutƒô
- Streaming nie jest jeszcze wspierany dla modeli Gemini
- Niekt√≥re zaawansowane funkcje OpenAI mogƒÖ nie dzia≈Çaƒá

### Og√≥lne
- Gateway automatycznie wybiera fallback gdy g≈Ç√≥wny model jest niedostƒôpny
- Cache pomaga zaoszczƒôdziƒá limity dla identycznych zapyta≈Ñ

## RozwiƒÖzywanie problem√≥w

1. **Timeout** - Zwiƒôksz MAX_RETRIES w pliku .env
2. **B≈Çƒôdy modelu** - Sprawd≈∫ logi, zweryfikuj poprawno≈õƒá klucza API
3. **Problemy z wydajno≈õciƒÖ** - Dostosuj ustawienia cache i retry

## Licencja

MIT
