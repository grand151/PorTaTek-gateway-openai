# Emulator API OpenAI z uÅ¼yciem darmowych modeli

Ten projekt to gateway (proxy), ktÃ³ry emuluje interfejs API OpenAI, przekierowujÄ…c zapytania do darmowych modeli dostÄ™pnych przez OpenRouter oraz Google Gemini API.

## Funkcje

- ğŸ”„ PeÅ‚na emulacja API OpenAI (drop-in replacement)
- ğŸ†“ Wykorzystanie darmowych modeli z OpenRouter i Google Gemini
- ğŸ—ºï¸ Automatyczne mapowanie modeli GPT na darmowe alternatywy
- ğŸš€ Wsparcie dla emulacji GPT-5 i najnowszych modeli
- ğŸ”„ Automatyczny retry w przypadku bÅ‚Ä™dÃ³w
- ğŸ”€ Fallback do alternatywnych modeli w przypadku awarii
- ğŸ’¾ Cachowanie odpowiedzi dla oszczÄ™dnoÅ›ci czasu i zasobÃ³w
- ğŸ–¼ï¸ ObsÅ‚uga modeli multimodalnych (tekst + obrazy)
- ğŸ¤– Wsparcie dla wielu providerÃ³w (OpenRouter, Google Gemini)
- ğŸ›ï¸ Panel konfiguracyjny z interfejsem webowym

## DostÄ™pne modele

### Modele z OpenRouter (darmowe)

#### DeepSeek (najnowsze, wydajne)
- **deepseek-r1-0528** - 164K context, Å›wietny dla reasoning i dialogu

#### Qwen (multimodalne, kodowanie)
- **qwen3-235b** - 262K context, zaawansowane reasoning
- **qwen3-next-80b** - 262K context, szybki i wszechstronny
- **qwen3-coder** - specjalizowany w kodowaniu
- **qwen3-vl-235b-thinking** - model wizyjny z thinking
- **qwen3-vl-30b-thinking** - model wizyjny, lÅ¼ejszy

#### Mistral AI
- **mistral-small-3.1-24b** - 128K context, vision, narzÄ™dzia
- **mistral-small-2501** - nowa wersja, fallback
- **mistral-embed** - embeddings

#### Meta Llama
- **llama-3.3-70b** - duÅ¼y model, wysokiej jakoÅ›ci
- **llama-3.2-3b** - szybki, lekki model

#### Google Gemma
- **gemma-2-9b** - open source, uniwersalny
- **gemma-2-2b** - najmniejszy, najszybszy

#### OpenCode (nowe modele)
- **opencode-big-pickle** - model opencode Big Pickle
- **opencode-glm-5** - model opencode GLM-5
- **opencode-gpt-5-nano** - model opencode GPT-5 Nano
- **opencode-kimi-k2.5** - model opencode Kimi K2.5
- **opencode-minimax-m2.5** - model opencode Minimax M2.5

### Modele Google Gemini (bezpoÅ›rednie API)

- **gemini-3-flash** - najnowszy, ultraszybki
- **gemini-3-pro** - najlepszy reasoning i analiza
- **gemini-2.0-flash** - do 1M tokenÃ³w context
- **gemini-1.5-flash** - szybki, 128K context
- **gemini-1.5-pro** - zaawansowany, 1M context

## Mapowanie modeli

| Model OpenAI | Model docelowy | Provider |
|--------------|----------------|----------|
| gpt-3.5-turbo | DeepSeek R1 | OpenRouter |
| gpt-4 | DeepSeek R1 | OpenRouter |
| gpt-4o | Qwen3 235B | OpenRouter |
| gpt-4o-mini | Qwen3 Next 80B | OpenRouter |
| **gpt-5** | **Qwen3 235B** | **OpenRouter** |
| **gpt-5-turbo** | **Qwen3 Next 80B** | **OpenRouter** |
| **gpt-5-nano** | **OpenCode GPT-5 Nano** | **OpenRouter** |
| **gpt-5-preview** | **Qwen3 235B** | **OpenRouter** |
| gpt-4-vision | Qwen3 VL 235B | OpenRouter |
| gpt-4-code | Qwen3 Coder | OpenRouter |
| opencode-big-pickle | OpenCode Big Pickle | OpenRouter |
| opencode-glm-5 | OpenCode GLM-5 | OpenRouter |
| opencode-gpt-5-nano | OpenCode GPT-5 Nano | OpenRouter |
| opencode-kimi-k2.5 | OpenCode Kimi K2.5 | OpenRouter |
| opencode-minimax-m2.5 | OpenCode Minimax M2.5 | OpenRouter |
| gemini-3-flash | Gemini 3 Flash | Google Gemini |
| gemini-1.5-pro | Gemini 1.5 Pro | Google Gemini |
| text-embedding-ada-002 | Mistral Embed | OpenRouter |


## Wymagania

- Node.js 14+
- npm lub yarn
- Klucz API OpenRouter (opcjonalny, jeÅ›li uÅ¼ywasz Gemini)
- Klucz API Google Gemini (opcjonalny, jeÅ›li uÅ¼ywasz OpenRouter)

**Uwaga:** Wymagany jest przynajmniej jeden klucz API (OpenRouter lub Gemini).

## Uzyskanie kluczy API

### OpenRouter API Key (darmowy)
1. Zarejestruj siÄ™ na [https://openrouter.ai](https://openrouter.ai)
2. PrzejdÅº do ustawieÅ„ konta
3. Wygeneruj nowy klucz API
4. Darmowe modele majÄ… limity: ~20 req/min, ~200 req/dzieÅ„

### Google Gemini API Key (darmowy)
1. OdwiedÅº [https://aistudio.google.com](https://aistudio.google.com)
2. Zaloguj siÄ™ kontem Google
3. Kliknij "Get API Key" lub "Create API key"
4. Skopiuj i bezpiecznie zapisz klucz
5. Darmowy tier: ~15 req/min (Flash), ~2-5 req/min (Pro)

## Instalacja

```bash
# Klonowanie repozytorium
git clone <repo-url>
cd openai-gateway

# Instalacja zaleÅ¼noÅ›ci
npm install

# Konfiguracja zmiennych Å›rodowiskowych
cp .env.example .env
# Edytuj plik .env z twoimi ustawieniami
```

## Uruchomienie

```bash
# Standardowe uruchomienie
npm start

# Tryb deweloperski z automatycznym restartem
npm run dev
```

Gateway bÄ™dzie dostÄ™pny pod adresem `http://localhost:8787`.

## Panel Konfiguracyjny

Gateway posiada wbudowany panel konfiguracyjny dostÄ™pny pod adresem `http://localhost:8787/admin`.

### Funkcje panelu:
- ğŸ“Š **Dashboard** - Status systemu, providery API, statystyki
- ğŸ­ **API Emulation** - PrzeÅ‚Ä…czanie miÄ™dzy emulacjÄ… OpenAI a niestandardowymi providerami
- ğŸ¤– **Modele** - PrzeglÄ…danie i dodawanie mapowaÅ„ modeli
- ğŸ”„ **Fallbacki** - Lista Å‚aÅ„cuchÃ³w fallbackÃ³w
- âš™ï¸ **Konfiguracja** - PeÅ‚ny widok konfiguracji JSON
- ğŸ“¡ **API Docs** - Dokumentacja endpointÃ³w i przykÅ‚ady uÅ¼ycia

### ZarzÄ…dzanie modelami przez API:

```bash
# Dodanie nowego mapowania modelu
curl -X POST "http://localhost:8787/config/models" \
  -H "Content-Type: application/json" \
  -d '{
    "openaiModel": "gpt-5-custom",
    "targetModel": "qwen/qwen3-235b-a22b:free",
    "provider": "openrouter"
  }'

# Pobranie aktualnej konfiguracji
curl "http://localhost:8787/config"

# Wyczyszczenie cache
curl -X POST "http://localhost:8787/config/clear-cache"
```

**Uwaga:** Zmiany konfiguracji przez panel sÄ… tymczasowe (tylko w pamiÄ™ci). Po restarcie serwera, konfiguracja wraca do wartoÅ›ci domyÅ›lnych.

## Emulacja API - PrzeÅ‚Ä…czanie miÄ™dzy providerami

Gateway obsÅ‚uguje dynamiczne przeÅ‚Ä…czanie miÄ™dzy API OpenAI a niestandardowymi providerami. Ta funkcja pozwala na rzeczywisty wybÃ³r implementacji API bez zmieniania kodu klienta.

### Dodawanie niestandardowego providera

```bash
# Dodanie nowego providera poprzez API
curl -X POST "http://localhost:8787/config/providers/custom" \
  -H "Content-Type: application/json" \
  -d '{
    "name": "my-provider",
    "displayName": "My Custom Provider",
    "endpoint": "https://api.myprovider.com/v1",
    "apiKeys": ["your-api-key"],
    "apiKeyHeader": "Authorization",
    "modelPrefix": "custom-"
  }'
```

### WyÅ›wietlanie dostÄ™pnych emulacji API

```bash
# Pobierz listÄ™ dostÄ™pnych API i bieÅ¼Ä…cÄ… emulacjÄ™
curl "http://localhost:8787/v1/emulate"
```

OdpowiedÅº:
```json
{
  "currentEmulation": "openai",
  "availableApis": [
    {
      "id": "openai",
      "name": "OpenAI API",
      "description": "Native OpenAI API emulation"
    },
    {
      "id": "my-provider",
      "name": "My Custom Provider",
      "endpoint": "https://api.myprovider.com/v1",
      "modelPrefix": "custom-"
    }
  ]
}
```

### PrzeÅ‚Ä…czanie emulacji API

```bash
# PrzeÅ‚Ä…cz na niestandardowy provider
curl -X POST "http://localhost:8787/v1/emulate" \
  -H "Content-Type: application/json" \
  -d '{"api": "my-provider"}'

# PrzeÅ‚Ä…cz z powrotem na OpenAI
curl -X POST "http://localhost:8787/v1/emulate" \
  -H "Content-Type: application/json" \
  -d '{"api": "openai"}'
```

### Dynamiczne modele podle providera

```bash
# Pobierz dostÄ™pne modele pogrupowane przez providera
curl "http://localhost:8787/v1/models-by-provider"
```

OdpowiedÅº zawiera modele dla kaÅ¼dego providera i ich prefiksy.

### ZarzÄ…dzanie emulacjÄ… w panelu

W panelu konfiguracyjnym (`/admin`), na karcie **API Emulation** moÅ¼esz:
- WyÅ›wietliÄ‡ listÄ™ wszystkich dostÄ™pnych API (OpenAI, custom providers)
- ZobaczyÄ‡, ktÃ³re API jest aktualnie emulowane
- KliknÄ…Ä‡ przycisk aby przeÅ‚Ä…czyÄ‡ emulacjÄ™ na inny provider
- PodglÄ…d endpointÃ³w i prefiksÃ³w modeli dla kaÅ¼dego providera

## UÅ¼ycie

MoÅ¼esz uÅ¼ywaÄ‡ tego gateway dokÅ‚adnie tak samo jak normalnego API OpenAI. Gateway automatycznie wykrywa i kieruje Å¼Ä…dania do odpowiedniego providera.

### PrzykÅ‚ad z modelami OpenRouter

```bash
curl -X POST "http://localhost:8787/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-4",
    "messages": [
        {"role": "user", "content": "WyjaÅ›nij jak dziaÅ‚a AI"}
    ]
  }'
```

### PrzykÅ‚ad z modelami Gemini

```bash
curl -X POST "http://localhost:8787/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gemini-3-flash",
    "messages": [
        {"role": "user", "content": "Co to jest machine learning?"}
    ]
  }'
```

### PrzykÅ‚ad z modelami kodowania

```bash
curl -X POST "http://localhost:8787/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-4-code",
    "messages": [
        {"role": "user", "content": "Napisz funkcjÄ™ sortujÄ…cÄ… w Python"}
    ]
  }'
```

### PrzykÅ‚ad z modelami OpenCode

```bash
curl -X POST "http://localhost:8787/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "opencode-big-pickle",
    "messages": [
        {"role": "user", "content": "WyjaÅ›nij koncepcjÄ™ machine learning"}
    ]
  }'
```

### PrzykÅ‚ad z modelami GPT-5 (emulacja)

```bash
# GPT-5 (mapowany na Qwen3 235B)
curl -X POST "http://localhost:8787/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-5",
    "messages": [
        {"role": "user", "content": "Co nowego w AI w 2026?"}
    ]
  }'

# GPT-5 Turbo (mapowany na Qwen3 Next 80B)
curl -X POST "http://localhost:8787/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-5-turbo",
    "messages": [
        {"role": "user", "content": "Szybka odpowiedÅº na pytanie"}
    ]
  }'

# GPT-5 Nano (mapowany na OpenCode GPT-5 Nano)
curl -X POST "http://localhost:8787/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-5-nano",
    "messages": [
        {"role": "user", "content": "Lekki i szybki model"}
    ]
  }'
```

### Python (OpenAI SDK)

```python
from openai import OpenAI

# Inicjalizacja klienta z nowym base URL
client = OpenAI(
    base_url="http://localhost:8787/v1",
    api_key="dowolny-string"  # klucz nie jest sprawdzany
)

# UÅ¼ycie dokÅ‚adnie jak z OpenAI
response = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[
        {"role": "user", "content": "Twoje pytanie"}
    ]
)

print(response.choices[0].message.content)
```

### JavaScript/TypeScript

```javascript
import OpenAI from "openai";

const client = new OpenAI({
  baseURL: "http://localhost:8787/v1",
  apiKey: "dowolny-string" // klucz nie jest sprawdzany
});

const response = await client.chat.completions.create({
  model: "gpt-3.5-turbo",
  messages: [
    { role: "user", content: "Twoje pytanie" }
  ]
});

console.log(response.choices[0].message.content);
```

## DostÄ™pne endpointy

- `/v1/chat/completions` - generowanie odpowiedzi czatu
- `/v1/embeddings` - generowanie embeddingÃ³w
- `/v1/models` - lista dostÄ™pnych modeli
- `/health` - sprawdzenie statusu serwera
- `/` - informacje o gateway
- `/admin` - panel konfiguracyjny (interfejs webowy)
- `/config` - pobieranie konfiguracji (JSON)
- `/config/models` - zarzÄ…dzanie mapowaniem modeli (POST)
- `/config/clear-cache` - czyszczenie cache (POST)

## Docker

```bash
# Budowanie obrazu
docker build -t openai-gateway .

# Uruchomienie
docker run -p 8787:8787 --env-file .env openai-gateway
```

MoÅ¼esz teÅ¼ uÅ¼yÄ‡ docker-compose:

```bash
docker-compose up -d
```

## Zmienne Å›rodowiskowe

| Zmienna | Opis | DomyÅ›lna wartoÅ›Ä‡ |
|---------|------|------------------|
| PORT | Port na ktÃ³rym dziaÅ‚a serwer | 8787 |
| OPENROUTER_API_KEY | Klucz API do OpenRouter | (opcjonalny*) |
| GEMINI_API_KEY | Klucz API do Google Gemini | (opcjonalny*) |
| CACHE_TTL | Czas Å¼ycia cache w milisekundach | 3600000 (1h) |
| MAX_RETRIES | Maksymalna liczba ponownych prÃ³b | 3 |
| RETRY_DELAY | OpÃ³Åºnienie miÄ™dzy prÃ³bami (ms) | 1000 |

*Wymagany przynajmniej jeden z kluczy API (OPENROUTER_API_KEY lub GEMINI_API_KEY)

## Limity i ograniczenia

### OpenRouter
- Darmowe modele mogÄ… byÄ‡ wolniejsze niÅ¼ oryginalne modele OpenAI
- Limity: ~20 zapytaÅ„/minutÄ™, ~200 zapytaÅ„/dzieÅ„
- DostÄ™pnoÅ›Ä‡ zaleÅ¼y od OpenRouter

### Google Gemini
- Gemini Flash: ~15 zapytaÅ„/minutÄ™
- Gemini Pro: ~2-5 zapytaÅ„/minutÄ™
- Streaming nie jest jeszcze wspierany dla modeli Gemini
- NiektÃ³re zaawansowane funkcje OpenAI mogÄ… nie dziaÅ‚aÄ‡

### OgÃ³lne
- Gateway automatycznie wybiera fallback gdy gÅ‚Ã³wny model jest niedostÄ™pny
- Cache pomaga zaoszczÄ™dziÄ‡ limity dla identycznych zapytaÅ„

## RozwiÄ…zywanie problemÃ³w

1. **Timeout** - ZwiÄ™ksz MAX_RETRIES w pliku .env
2. **BÅ‚Ä™dy modelu** - SprawdÅº logi, zweryfikuj poprawnoÅ›Ä‡ klucza API
3. **Problemy z wydajnoÅ›ciÄ…** - Dostosuj ustawienia cache i retry

## Licencja

MIT
